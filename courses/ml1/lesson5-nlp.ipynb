{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.nlp import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset and the sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
    "\n",
    "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n",
    "\n",
    "To get the dataset, in your terminal run the following commands:\n",
    "\n",
    "`wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n",
    "\n",
    "`gunzip aclImdb_v1.tar.gz`\n",
    "\n",
    "`tar -xvf aclImdb_v1.tar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and term document matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/aclImdb/'\n",
    "names = ['neg','pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\r\n",
      "\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_9.txt\n",
      "10000_8.txt\n",
      "10001_10.txt\n",
      "10002_7.txt\n",
      "10003_8.txt\n",
      "10004_8.txt\n",
      "10005_7.txt\n",
      "10006_7.txt\n",
      "10007_7.txt\n",
      "10008_7.txt\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}train/pos | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\n",
    "val,val_y = texts_labels_from_folders(f'{PATH}test',names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the text of the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "??texts_labels_from_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I disagree with much that has been written and said about this supposed \"masterpiece\" of the German New Wave: <br /><br />1) There are major flaws in simple exposition, in the basic communication of critical plot points, as relating to Maria\\'s abortion and the secret contract between Oswald and her husband. How many viewers understood that the husband agreed, in exchange for substantial financial remuneration, not to return to and reclaim his wife until Oswald was dead?<br /><br />2) The ending is highly unsatisfying because arbitrary and accidental. The original screenplay called for Maria to commit suicide after the reading of Oswald\\'s will, on finding out that her husband had in effect sold their marriage to Oswald. In the final version, however, Maria only runs water from a faucet across her wrist in a gesture of suicide. Maria is then summarily blown up, rather than having to confront and live with the consequences of her self-delusion and moral compromise.<br /><br />3) Fassbinder seeks to forcibly superimpose the public on the private, the political on the personal. Contrary to what the critics and \"experts\" assert, I don\\'t think it works. Merely intruding historic radio news or the sound of the jackhammers of German reconstruction in the soundtrack on the dramatic events of the movie does not make those historical events integral to the drama. <br /><br />The selfish ambition of Maria\\'s rise from poverty to prosperity is meant to parallel the so-called economic miracle of postwar Germany. Maria is thus intended to be a woman specific to and reflective of her time and place, but is in reality unoriginal and nonspecific. Women have been asserting their independence by using sex for self-advancement for ages. <br /><br />4) Lastly, there are several instances of inexcusable sloppiness and amateurishness -- Fassbinder\\'s drug addiction and consequent impatience and inattention have had their effect. Unknown people talk off screen without ever being seen; music is clumsily intrusive in places; and melodramatic posturing sporadically substitutes for acting.<br /><br />Strangely, for a movie condemning a country for willful collective amnesia of the holocaust, it itself never mentions it once.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. `trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 237 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aussie', 'aussies', 'austen', 'austeniana', 'austens']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = veczr.get_feature_names(); vocab[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"experts\"',\n",
       " '\"masterpiece\"',\n",
       " '--',\n",
       " '/>1)',\n",
       " '/>2)',\n",
       " '/>3)',\n",
       " '/>4)',\n",
       " '/><br',\n",
       " '/>strangely,',\n",
       " '/>the',\n",
       " '<br',\n",
       " 'a',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'accidental.',\n",
       " 'across',\n",
       " 'acting.<br',\n",
       " 'addiction',\n",
       " 'after',\n",
       " 'ages.',\n",
       " 'agreed,',\n",
       " 'amateurishness',\n",
       " 'ambition',\n",
       " 'amnesia',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'are',\n",
       " 'as',\n",
       " 'assert,',\n",
       " 'asserting',\n",
       " 'basic',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'being',\n",
       " 'between',\n",
       " 'blown',\n",
       " 'but',\n",
       " 'by',\n",
       " 'called',\n",
       " 'clumsily',\n",
       " 'collective',\n",
       " 'commit',\n",
       " 'communication',\n",
       " 'compromise.<br',\n",
       " 'condemning',\n",
       " 'confront',\n",
       " 'consequences',\n",
       " 'consequent',\n",
       " 'contract',\n",
       " 'contrary',\n",
       " 'country',\n",
       " 'critical',\n",
       " 'critics',\n",
       " 'dead?<br',\n",
       " 'disagree',\n",
       " 'does',\n",
       " \"don't\",\n",
       " 'drama.',\n",
       " 'dramatic',\n",
       " 'drug',\n",
       " 'economic',\n",
       " 'effect',\n",
       " 'effect.',\n",
       " 'ending',\n",
       " 'events',\n",
       " 'ever',\n",
       " 'exchange',\n",
       " 'exposition,',\n",
       " 'fassbinder',\n",
       " \"fassbinder's\",\n",
       " 'faucet',\n",
       " 'final',\n",
       " 'financial',\n",
       " 'finding',\n",
       " 'flaws',\n",
       " 'for',\n",
       " 'forcibly',\n",
       " 'from',\n",
       " 'german',\n",
       " 'germany.',\n",
       " 'gesture',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'her',\n",
       " 'highly',\n",
       " 'his',\n",
       " 'historic',\n",
       " 'historical',\n",
       " 'holocaust,',\n",
       " 'how',\n",
       " 'however,',\n",
       " 'husband',\n",
       " 'husband.',\n",
       " 'i',\n",
       " 'impatience',\n",
       " 'in',\n",
       " 'inattention',\n",
       " 'independence',\n",
       " 'inexcusable',\n",
       " 'instances',\n",
       " 'integral',\n",
       " 'intended',\n",
       " 'intruding',\n",
       " 'intrusive',\n",
       " 'is',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'jackhammers',\n",
       " 'lastly,',\n",
       " 'live',\n",
       " 'major',\n",
       " 'make',\n",
       " 'many',\n",
       " 'maria',\n",
       " \"maria's\",\n",
       " 'marriage',\n",
       " 'meant',\n",
       " 'melodramatic',\n",
       " 'mentions',\n",
       " 'merely',\n",
       " 'miracle',\n",
       " 'moral',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'music',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'nonspecific.',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once.',\n",
       " 'only',\n",
       " 'or',\n",
       " 'original',\n",
       " 'oswald',\n",
       " \"oswald's\",\n",
       " 'oswald.',\n",
       " 'out',\n",
       " 'parallel',\n",
       " 'people',\n",
       " 'personal.',\n",
       " 'place,',\n",
       " 'places;',\n",
       " 'plot',\n",
       " 'points,',\n",
       " 'political',\n",
       " 'posturing',\n",
       " 'postwar',\n",
       " 'poverty',\n",
       " 'private,',\n",
       " 'prosperity',\n",
       " 'public',\n",
       " 'radio',\n",
       " 'rather',\n",
       " 'reading',\n",
       " 'reality',\n",
       " 'reclaim',\n",
       " 'reconstruction',\n",
       " 'reflective',\n",
       " 'relating',\n",
       " 'remuneration,',\n",
       " 'return',\n",
       " 'rise',\n",
       " 'runs',\n",
       " 'said',\n",
       " 'screen',\n",
       " 'screenplay',\n",
       " 'secret',\n",
       " 'seeks',\n",
       " 'seen;',\n",
       " 'self-advancement',\n",
       " 'self-delusion',\n",
       " 'selfish',\n",
       " 'several',\n",
       " 'sex',\n",
       " 'simple',\n",
       " 'sloppiness',\n",
       " 'so-called',\n",
       " 'sold',\n",
       " 'sound',\n",
       " 'soundtrack',\n",
       " 'specific',\n",
       " 'sporadically',\n",
       " 'substantial',\n",
       " 'substitutes',\n",
       " 'suicide',\n",
       " 'suicide.',\n",
       " 'summarily',\n",
       " 'superimpose',\n",
       " 'supposed',\n",
       " 'talk',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'then',\n",
       " 'there',\n",
       " 'think',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thus',\n",
       " 'time',\n",
       " 'to',\n",
       " 'understood',\n",
       " 'unknown',\n",
       " 'unoriginal',\n",
       " 'unsatisfying',\n",
       " 'until',\n",
       " 'up,',\n",
       " 'using',\n",
       " 'version,',\n",
       " 'viewers',\n",
       " 'was',\n",
       " 'water',\n",
       " 'wave:',\n",
       " 'what',\n",
       " 'wife',\n",
       " 'will,',\n",
       " 'willful',\n",
       " 'with',\n",
       " 'without',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'works.',\n",
       " 'wrist',\n",
       " 'written'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = set([o.lower() for o in trn[0].split(' ')]); w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['absurd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,1297]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the **log-count ratio** $r$ for each word $f$:\n",
    "\n",
    "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
    "\n",
    "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=trn_y\n",
    "\n",
    "r = np.log(pr(1)/pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the formula for Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81656"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and binarized Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83016"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=trn_term_doc.sign()\n",
    "r = np.log(pr(1)/pr(0))\n",
    "\n",
    "pre_preds = val_term_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can fit logistic regression where the features are the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidd/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/sidd/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8324"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85516"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the regularized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84872"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88404"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram with NB features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=trn_y\n",
    "x=trn_term_doc.sign()\n",
    "val_x = val_term_doc.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr(1) / pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidd/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True)\n",
    "m.fit(x, y);\n",
    "\n",
    "preds = m.predict(val_x)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the $\\text{log-count ratio}$ `r`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 800000),\n",
       " matrix([[-0.05468, -0.161  , -0.24784, ...,  1.09861, -0.69315, -0.69315]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.94678, 0.85129, 0.78049, ..., 3.     , 0.5    , 0.5    ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "m = LogisticRegression(dual=True, C=0.1)\n",
    "m.fit(x_nb, y);\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai NBSVM++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sl ... max sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we get a model from a bag of words\n",
    "md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59f0c58ed6942acb5f8c9c389decba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.02507501437651315 [0.11947]  0.9167600000190735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11947]), 0.9167600000190735]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = md.dotprod_nb_learner()\n",
    "learner.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42e106781564a8d9eb279b6dead42af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.02127116495933404 [0.11349]  0.9208400000190735\n",
      "    1      0.011063079297005058 [0.11142]  0.9217200000190735 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11142]), 0.9217200000190735]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd4a80948e412bb5443b1ed05193c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>                     \n",
      "    0      0.01881257667500745 [0.11001]  0.9222800000190735\n",
      "    1      0.010419971695920082 [0.10903]  0.9222800000190735  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.10903]), 0.9222800000190735]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
